<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Learn More – Synthetic Image Detection</title>

<style>
    * {
        box-sizing: border-box;
        font-family: "Segoe UI", Arial, sans-serif;
        scroll-behavior: smooth;
    }

    body {
        margin: 0;
        background: #f5f3ff; /* light purple background */
        padding: 20px;
        color: #1f2937;
    }

    .container {
        max-width: 1100px;
        margin: 0 auto;
        background: #ffffff;
        padding: 30px 50px 50px 50px;
        border-radius: 16px;
        box-shadow: 0 12px 40px rgba(126, 34, 206, 0.15); /* subtle purple shadow */
    }

    /* ================= TITLE ================= */
    .title {
        text-align: center;
        margin-bottom: 50px;
    }

    .title h1 {
        color: #7e22ce; /* purple heading */
        font-size: 40px;
        font-weight: 700;
        text-shadow: 1px 1px 10px rgba(126, 34, 206, 0.3);
        margin-bottom: 10px;
    }

    .title p {
        color: #6b7280;
        font-size: 17px;
    }

    /* ================= WORKFLOW IMAGE ================= */
    .workflow-image {
        display: flex;
        justify-content: center;
        margin: 40px 0;
    }

    .workflow-image img {
        max-width: 100%;
        width: 800px;
        border-radius: 16px;
        box-shadow: 0 8px 20px rgba(126, 34, 206, 0.25);
        transition: transform 0.3s ease, box-shadow 0.3s ease;
    }

    .workflow-image img:hover {
        transform: scale(1.03);
        box-shadow: 0 12px 28px rgba(126, 34, 206, 0.35);
    }

    /* ================= DESCRIPTION ================= */
    .description {
        margin-top: 30px;
        color: #374151;
        line-height: 1.8;
        font-size: 17px;
        text-align: justify;
    }

    .description p {
        margin-bottom: 20px;
    }

    .description strong {
        color: #7e22ce; /* highlight key terms in purple */
        font-weight: 600;
    }

    /* ================= FAQ ================= */
    .faq-section {
        margin-top: 60px;
    }

    .faq-section h2 {
        text-align: center;
        color: #7e22ce;
        font-size: 32px;
        margin-bottom: 30px;
        text-shadow: 1px 1px 5px rgba(126, 34, 206, 0.2);
    }

    .faq {
        background: #faf5ff;
        border-radius: 12px;
        margin-bottom: 14px;
        overflow: hidden;
        box-shadow: 0 6px 15px rgba(126, 34, 206, 0.1);
        transition: all 0.3s ease;
        cursor: pointer;
    }

    .faq:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 20px rgba(126, 34, 206, 0.2);
    }

    .faq-question {
        padding: 18px 24px;
        font-weight: 600;
        font-size: 17px;
        color: #1f2937;
        display: flex;
        justify-content: space-between;
        align-items: center;
    }

    .faq-question span {
        font-size: 24px;
        transition: transform 0.3s ease;
        color: #7e22ce;
    }

    .faq-answer {
        max-height: 0;
        overflow: hidden;
        padding: 0 24px;
        background: #ffffff;
        transition: max-height 0.4s ease, padding 0.4s ease;
    }

    .faq-answer p {
        padding: 14px 0;
        margin: 0;
        color: #4b5563;
        font-size: 16px;
    }

    .faq.active .faq-answer {
        max-height: 300px;
        padding: 14px 24px;
    }

    .faq.active .faq-question span {
        transform: rotate(180deg);
    }

    /* ================= BACK LINK ================= */
    .back-link {
        text-align: center;
        margin-top: 50px;
    }

    .back-link a {
        text-decoration: none;
        color: #ffffff;
        font-weight: 600;
        background-color: #7e22ce;
        padding: 10px 20px;
        border-radius: 8px;
        transition: all 0.3s ease;
    }

    .back-link a:hover {
        background-color: #a855f7;
        box-shadow: 0 6px 15px rgba(126, 34, 206, 0.4);
        transform: translateY(-2px);
    }

    /* ================= MOBILE ================= */
    @media (max-width: 768px) {
        .container {
            padding: 25px 15px 40px 15px;
        }
        .title h1 {
            font-size: 28px;
        }
        .workflow-image img {
            width: 100%;
        }
        .description {
            font-size: 16px;
        }
        .faq-question {
            font-size: 16px;
        }
        .faq-answer p {
            font-size: 15px;
        }
    }

</style>
</head>

<body>

<div class="container">

    <!-- TITLE -->
    <div class="title">
        <h1>Working of Synthetic Image Detection</h1>
        <p>Understanding how uploaded images are analyzed and classified</p>
    </div>

    <!-- WORKFLOW IMAGE -->
    <div class="workflow-image">
        <img src="static/extra_images/learnmore.png" alt="Synthetic Image Detection Workflow">
    </div>

    <!-- DESCRIPTION -->
    <div class="description">
        <p>
            The workflow begins when a user uploads an image into the system. Once the image
            is received, it undergoes a series of automated processing steps that prepare it
            for analysis. These steps ensure the image is normalized, resized, and optimized
            for efficient feature extraction.
        </p>

        <p>
            At the core of the detection process lies a <strong>Convolutional Neural Network (CNN)</strong>,
            which is specifically designed to learn spatial and visual patterns from images.
            CNNs are highly effective at identifying subtle inconsistencies in pixel structure,
            lighting behavior, and texture distribution—characteristics that often differentiate
            real images from synthetic ones.
        </p>

        <p>
            Our system leverages <strong>MobileNetV2</strong> as the backbone architecture.
            MobileNetV2 is a lightweight yet powerful deep learning model optimized for fast
            inference and low computational cost. This allows the detector to deliver
            real-time results while maintaining high accuracy, even on devices with limited
            hardware resources.
        </p>

        <p>
            After processing, the extracted features are evaluated and the image is classified
            into one of two categories: <strong>Real</strong> or <strong>Synthetic</strong>.
            The final output is presented clearly to the user, enabling quick and reliable
            decision-making.
        </p>
    </div>

    <!-- FAQ SECTION -->
    <div class="faq-section">
        <h2>Frequently Asked Questions</h2>

        <div class="faq">
            <div class="faq-question">
                What is synthetic image detection?
                <span>⌄</span>
            </div>
            <div class="faq-answer">
                <p>
                    Synthetic image detection identifies whether an image is generated by AI models
                    or captured from real-world sources by analyzing visual and structural patterns.
                </p>
            </div>
        </div>

        <div class="faq">
            <div class="faq-question">
                Which deep learning architecture is used?
                <span>⌄</span>
            </div>
            <div class="faq-answer">
                <p>
                    The system uses a Convolutional Neural Network (CNN) with MobileNetV2 as the
                    backbone architecture for efficient feature extraction and classification.
                </p>
            </div>
        </div>

        <div class="faq">
            <div class="faq-question">
                Why MobileNetV2 instead of heavier models?
                <span>⌄</span>
            </div>
            <div class="faq-answer">
                <p>
                    MobileNetV2 offers a lightweight design with depthwise separable convolutions,
                    enabling faster inference while maintaining high accuracy.
                </p>
            </div>
        </div>

        <div class="faq">
            <div class="faq-question">
                What features does the model analyze?
                <span>⌄</span>
            </div>
            <div class="faq-answer">
                <p>
                    The model analyzes pixel-level inconsistencies, texture irregularities,
                    lighting artifacts, and facial symmetry patterns commonly found in AI-generated images.
                </p>
            </div>
        </div>

        <div class="faq">
            <div class="faq-question">
                What types of AI-generated images can be detected?
                <span>⌄</span>
            </div>
            <div class="faq-answer">
                <p>
                    The detector is optimized for human face images generated using GANs
                    and diffusion-based models.
                </p>
            </div>
        </div>

        <div class="faq">
            <div class="faq-question">
                Is the model trained on real-world data?
                <span>⌄</span>
            </div>
            <div class="faq-answer">
                <p>
                    Yes. The model is trained on a combination of real human face images
                    and AI-generated samples to improve generalization.
                </p>
            </div>
        </div>

        <div class="faq">
            <div class="faq-question">
                How accurate is the prediction?
                <span>⌄</span>
            </div>
            <div class="faq-answer">
                <p>
                    Accuracy depends on image quality and dataset diversity, but the model
                    achieves strong performance on benchmark human face datasets.
                </p>
            </div>
        </div>

    </div>

    <div class="back-link">
        <a href="/">← Back to Home</a>
    </div>

</div>

<script>
    // FAQ toggle
    document.querySelectorAll(".faq-question").forEach(q => {
        q.addEventListener("click", () => {
            q.parentElement.classList.toggle("active");
        });
    });
</script>

</body>
</html>
